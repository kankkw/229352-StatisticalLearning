{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kankkw/229352-StatisticalLearning/blob/main/Lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE5JHoY48UdL"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #9\n",
        "\n",
        "[Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/donlapark/ds352-labs.git"
      ],
      "metadata": {
        "id": "xgWqscGrMKlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzJSrt8CSv4p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a neural network in PyTorch"
      ],
      "metadata": {
        "id": "BcbkxmWA74vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chihuahua or Muffin?\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/lab09-preview.jpg\" width=\"500\"/></center>"
      ],
      "metadata": {
        "id": "nGIdhbtd8T2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data preparation"
      ],
      "metadata": {
        "id": "327TUVAY-GWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load images, resize them to 128x128, and normalize the pixels to be in 0 - 1 range"
      ],
      "metadata": {
        "id": "zOBcEplu9fen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkI3ITOLt2S6"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                                transforms.ToTensor()])  # transform pixels to be in 0 - 1 range\n",
        "\n",
        "dataset = datasets.ImageFolder(root=\"ds352-labs/lab09-data/train\",\n",
        "                                         transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the dataset into training (80%), validation (20%)"
      ],
      "metadata": {
        "id": "y_NMJJiW9w7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "oPI8NPnp9ctV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the datasets into DataLoader"
      ],
      "metadata": {
        "id": "ToRdaB3M90qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=10,\n",
        "                          shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset,\n",
        "                        batch_size=len(val_dataset),\n",
        "                        shuffle=False)"
      ],
      "metadata": {
        "id": "kVPWfJt_900l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do the same for the test images"
      ],
      "metadata": {
        "id": "DXWUYAe398Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.ImageFolder(root=\"ds352-labs/lab09-data/test\",\n",
        "                                    transform=transform)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=len(test_dataset),\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "0DE9UZxZ-B9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Looking at the first minibatch"
      ],
      "metadata": {
        "id": "bAr3zICjAQ7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = iter(train_loader)\n",
        "X, y = next(train_batches)\n",
        "\n",
        "print(X.shape)  # (batch_size, channel, height, weight)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "DsbiahaH_ynd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the first four images in the batch"
      ],
      "metadata": {
        "id": "4pKwuqvIA7Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X[:4]  # Select the first 4 images\n",
        "X = X.numpy().transpose(0, 2, 3, 1)  # Convert from (B, C, H, W) to (B, H, W, C)\n",
        "\n",
        "# Plot images\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
        "for i in range(4):\n",
        "    axes[i].imshow(X[i])\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(y[:4])"
      ],
      "metadata": {
        "id": "zQDD_t91AZ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Build a simple logistic regression\n",
        "\n",
        "<center><img src=\"https://donlapark.pages.dev/229352/logistic.png\" width=\"300\"/></center>"
      ],
      "metadata": {
        "id": "hr4ZT0OFA_uV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important component of the model class is the `__init__` method and the `forward` method.  \n",
        "\n",
        "[Linear layer in Pytorch](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "\n",
        "[Activation functions in PyTorch](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). The most important ones are [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html), [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html), [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html), [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html)."
      ],
      "metadata": {
        "id": "k_ZfXfKCDFkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleLogisticRegression, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(3*128*128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_FPkDKi9CnyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Initialize training components"
      ],
      "metadata": {
        "id": "W-CprLjPBYl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the model and loss function"
      ],
      "metadata": {
        "id": "tkEoVjYjCdfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Loss functions in PyTorch](https://pytorch.org/docs/stable/nn.html#loss-functions). Most important ones are [MSE](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html), [Binary cross, entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), [Categorical cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
      ],
      "metadata": {
        "id": "r9sIWVitYoK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleLogisticRegression()\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "PGJAjLqUfIwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manually setting initial weights to zero for demonstration"
      ],
      "metadata": {
        "id": "jLa6AhMZBcZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for layer in model.modules():\n",
        "      if isinstance(layer, nn.Linear):\n",
        "          layer.weight.zero_()\n",
        "          layer.bias.zero_()"
      ],
      "metadata": {
        "id": "x9v3LrAfBci2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create two lists to collect training and validation losses"
      ],
      "metadata": {
        "id": "pwf_EFyvBrWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the loss values for plotting\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "WH7y788rBrhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Specify the learning rate"
      ],
      "metadata": {
        "id": "DrYHJN5tFjCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "4hvUj4ZcFjKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training the model with gradient descent"
      ],
      "metadata": {
        "id": "KB0gOHHrFsLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert the dataloader into minibatches"
      ],
      "metadata": {
        "id": "YSA2AqK7F2E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train_loader:\n",
        "    break"
      ],
      "metadata": {
        "id": "Slbap3CpFsfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make a prediction on the minibatch (Forward pass)"
      ],
      "metadata": {
        "id": "ZPSgs0ZDF9bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model(X)\n",
        "y_hat = y_hat[:, 0]\n",
        "y = y.float()"
      ],
      "metadata": {
        "id": "V-Kgb2zhGteA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate the loss function"
      ],
      "metadata": {
        "id": "vzyJ9MyQGto1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that `criterion()` is our binary cross-entropy loss (`BCELoss`)."
      ],
      "metadata": {
        "id": "iIvXv7a5H0tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the loss\n",
        "loss = criterion(y_hat, y)\n",
        "train_losses.append(loss.item())"
      ],
      "metadata": {
        "id": "RclJ1jKiGtyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j04pTTnZcAQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate the gradient (Backward pass)"
      ],
      "metadata": {
        "id": "H3k4jXI4Gt6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward pass: compute the gradient of the loss w.r.t. model parameters\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "h39QSy3EGuCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perform a gradient descent step"
      ],
      "metadata": {
        "id": "hUNXcIxMG5Sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Careful! We must not include this step in the gradient calculation, hence the use of `with torch.no_grad()`."
      ],
      "metadata": {
        "id": "9pG-9dE8IVIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the weights using the gradient descent rule\n",
        "\n",
        "\n",
        "# Zero the gradients after updating\n",
        "optimizer.step()\n",
        "model.zero_grad()"
      ],
      "metadata": {
        "id": "INEXFZDNG5bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do the same for the validation set"
      ],
      "metadata": {
        "id": "TFWFz6qkITT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Careful! Anything in the validation step must not be included in the gradient calculation, hence the use of `with torch.no_grad()`."
      ],
      "metadata": {
        "id": "HoYwFcRxIghc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for X, y in val_loader:\n",
        "        y = y.float()\n",
        "        y_hat = model(X)\n",
        "        y_hat = y_hat[:, 0]\n",
        "        val_loss = criterion(y_hat, y)\n",
        "        val_losses.append(val_loss.item())"
      ],
      "metadata": {
        "id": "yNON7t68gH3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_losses)\n",
        "print(val_losses)"
      ],
      "metadata": {
        "id": "ue8xC7LMV2m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combine everything together."
      ],
      "metadata": {
        "id": "XEGS9Gi5JB5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the previous steps for 20 **epochs** and plot the training and validation losses."
      ],
      "metadata": {
        "id": "2wxuMkbbJVmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleLogisticRegression()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for layer in model.modules():\n",
        "      if isinstance(layer, nn.Linear):\n",
        "          layer.weight.zero_()\n",
        "          layer.bias.zero_()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "\n",
        "    model.train()\n",
        "    for X, y in train_loader:\n",
        "        y = y.float()\n",
        "        y_hat = model(X)\n",
        "        y_hat = y_hat[:, 0]\n",
        "\n",
        "        loss = criterion(y_hat, y)\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            y = y.float()\n",
        "            y_hat = model(X)\n",
        "            y_hat = y_hat[:, 0]\n",
        "\n",
        "            val_loss = criterion(y_hat, y)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")"
      ],
      "metadata": {
        "id": "vmOxV1m5JCNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZBZ3f66AFye"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "In this exercise, we will add more layers to our classification model.\n",
        "\n",
        "<img src=\"https://donlapark.pages.dev/229352/lab09-architecture.png\" width=\"450\"/>\n",
        "\n",
        "1. Create a neural network with 3 hidden layers as shown in the picture.\n",
        "\n",
        "2. Train the model with learning rate = 1e-2, 1e-3, 1e-4, 1e-5, and answer the following questions.\n",
        "    2.1 What value of learning rate do you **think** is the best? Please explain your reason.\n",
        "    2.2 What happens to the training losses if your learning rate is too large?\n",
        "    2.3 What happens to the training losses if your learning rate is too small?\n",
        "\n",
        "3. After finish training your model. Make the predictions on the test set and compute the accuracy. You may use the provided code below.\n",
        "\n",
        "4. Use `plt.imshow()` to display at least four images that are incorrectly classified by this model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3*128*128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "chMdDfejhbVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1\n",
        "I think 1e-3 is the best learning rate because it converges faster than 1e-4 and 1e-5,\n",
        "while still maintaining stable training compared to 1e-2 which may cause oscillation."
      ],
      "metadata": {
        "id": "hTJQmTeUkk8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2\n",
        "If the learning rate is too large, the training loss may oscillate or even diverge.\n",
        "The model may overshoot the minimum and fail to converge."
      ],
      "metadata": {
        "id": "4fw1PZsokp86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3\n",
        "If the learning rate is too small, the training loss decreases very slowly.\n",
        "The model may take too long to converge and may get stuck in local minima."
      ],
      "metadata": {
        "id": "2QmMczaTkvC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this code to calculate test accuracy\n",
        "with torch.no_grad():\n",
        "  test_batches = iter(test_loader)\n",
        "  X, y = next(test_batches)\n",
        "\n",
        "  y_hat = model(X)\n",
        "  y_hat = y_hat[:, 0]\n",
        "\n",
        "  y_hat = torch.sigmoid(y_hat)\n",
        "  y_hat = (y_hat > 0.5).float() # the prediction\n",
        "  ##TODO: compute accuracy\n",
        "  accuracy = (y_hat == y).float().mean()\n",
        "  print(\"Test Accuracy:\", accuracy.item())"
      ],
      "metadata": {
        "id": "X3-On4zBlTmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.float()\n",
        "\n",
        "incorrect = y_hat != y\n",
        "indices = torch.where(incorrect)[0][:4]\n",
        "\n",
        "for idx in indices:\n",
        "    img = X[idx].permute(1,2,0).numpy()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"True: {int(y[idx].item())}, Pred: {int(y_hat[idx].item())}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KJyNfeY5hkPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "name": "Lab09_Neural_Networks.ipynb",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}