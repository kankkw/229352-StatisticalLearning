{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kankkw/229352-StatisticalLearning/blob/main/Lab01_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X33DsAOeokmy"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "# For Fashion-MNIST\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# For 20 Newsgroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "JOnoRSjo8dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Marketing Campaign Dataset - Manual Data Preprocessing & Logistic Regression"
      ],
      "metadata": {
        "id": "exuopfRD8mHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Marketing Campaign Dataset ([Data Information](https://archive.ics.uci.edu/dataset/222/bank+marketing))\n",
        "\n",
        "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (`'yes'`) or not (`'no'`) subscribed."
      ],
      "metadata": {
        "id": "FdWsJYcVBf1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank_url = 'https://raw.githubusercontent.com/donlap/ds352-labs/main/bank.csv'\n",
        "\n",
        "df = pd.read_csv(bank_url, sep=';', na_values=['unknown'])\n",
        "df = df.drop([\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\t\"euribor3m\", \"nr.employed\"], axis=1)\n",
        "print(\"Shape of the dataset:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CJ9ujPUBBcTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration"
      ],
      "metadata": {
        "id": "x4invEyyBRDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Missing Values Count ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "u6wANpgzBQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Unique Values for Categorical Columns ---\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"\\n'{col}' unique values:\")\n",
        "    print(df[col].value_counts(dropna=False)) # Include NaN counts"
      ],
      "metadata": {
        "id": "1ack00iLnuCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "iCEtO_EWDTzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map target variable 'y' to 0 (no) and 1 (yes)\n",
        "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "# Drop 'duration' due to data leakage\n",
        "df = df.drop(columns=['duration'])\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']\n",
        "\n",
        "# Split the data BEFORE any transformations\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Print data shape\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape :\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape :\", y_test.shape)"
      ],
      "metadata": {
        "id": "GOb9VB7shrlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply `StandardScaler()`, `OrdinalEncoder()`, and `OneHotEncoder()` on a few selected columns."
      ],
      "metadata": {
        "id": "yM_gRkNjGR53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Numerical Feature: `age` and `campaign` (Standard Scaling)**"
      ],
      "metadata": {
        "id": "r4sPr5qDGY65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_demo = ['age', 'campaign']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler ONLY on the training data\n",
        "scaler.fit(X_train[num_cols_demo])\n",
        "\n",
        "# Transform both training and test data\n",
        "X_train_scaled_demo = scaler.transform(X_train[num_cols_demo])\n",
        "X_test_scaled_demo = scaler.transform(X_test[num_cols_demo])\n",
        "\n",
        "print(\"Scaled training data (first 5 rows):\")\n",
        "print(X_train_scaled_demo[:5])\n",
        "print(\"\\nScaled test data (first 5 rows):\")\n",
        "print(X_test_scaled_demo[:5])"
      ],
      "metadata": {
        "id": "DcyjTyGghyz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Ordinal Feature: `education` (Ordinal Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "8jQi5cJAGj_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord_col_demo = ['education']\n",
        "\n",
        "imputer_ord = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "imputer_ord.fit(X_train[ord_col_demo])\n",
        "\n",
        "X_train_imputed_ord_demo = imputer_ord.transform(X_train[ord_col_demo])\n",
        "X_test_imputed_ord_demo = imputer_ord.transform(X_test[ord_col_demo])\n",
        "\n",
        "print(\"Imputed 'education' (train) first 5 rows:\")\n",
        "print(X_train_imputed_ord_demo[:5])\n",
        "print(\"\\nImputed 'education' (test) first 5 rows:\")\n",
        "print(X_test_imputed_ord_demo[:5])"
      ],
      "metadata": {
        "id": "sEwMaRFIh5Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "2uyk1rA7K7_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Ordinal Encoding**"
      ],
      "metadata": {
        "id": "P9hDuDPyLGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "education_categories = [\n",
        "    'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school',\n",
        "    'professional.course', 'university.degree', 'masters', 'doctorate'\n",
        "]"
      ],
      "metadata": {
        "id": "n7DMoQ6AGf8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder = OrdinalEncoder(categories=[education_categories])\n",
        "\n",
        "ordinal_encoder.fit(X_train_imputed_ord_demo)\n",
        "\n",
        "X_train_encoded_ord_demo = ordinal_encoder.transform(X_train_imputed_ord_demo)\n",
        "X_test_encoded_ord_demo = ordinal_encoder.transform(X_test_imputed_ord_demo)\n",
        "\n",
        "print(\"Encoded 'education' (train) first 5 rows:\")\n",
        "print(X_train_encoded_ord_demo[:5])\n",
        "print(\"\\nEncoded 'education' (test) first 5 rows:\")\n",
        "print(X_test_encoded_ord_demo[:5])"
      ],
      "metadata": {
        "id": "36OXLTWViDNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Nominal Feature: `job` (One-Hot Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "GarEnu8iK1lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "1eJSiEslLNi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nom_col_demo = ['job']\n",
        "\n",
        "imputer_nom = SimpleImputer(strategy='most_frequent')\n",
        "imputer_nom.fit(X_train[nom_col_demo])\n",
        "\n",
        "X_train_imputed_nom_demo = imputer_nom.transform(X_train[nom_col_demo])\n",
        "X_test_imputed_nom_demo = imputer_nom.transform(X_test[nom_col_demo])"
      ],
      "metadata": {
        "id": "1OUbef1bJPUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Nominal Encoding**"
      ],
      "metadata": {
        "id": "l7F_kim6TQ72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "onehot_encoder.fit(X_train_imputed_nom_demo)\n",
        "\n",
        "X_train_onehot_nom_demo = onehot_encoder.transform(X_train_imputed_nom_demo)\n",
        "X_test_onehot_nom_demo = onehot_encoder.transform(X_test_imputed_nom_demo)\n",
        "\n",
        "print(\"One-hot encoded 'job' (train) shape:\", X_train_onehot_nom_demo.shape)\n",
        "print(\"One-hot encoded 'job' (test) shape :\", X_test_onehot_nom_demo.shape)"
      ],
      "metadata": {
        "id": "s6RneopLiXQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Apply All Preprocessing & Train Logistic Regression**\n",
        "\n",
        "Now, it's your turn to apply these preprocessing steps to *all* relevant columns and then train a Logistic Regression model.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  Look at the Variable Table in [this link](https://archive.ics.uci.edu/dataset/222/bank+marketing).\n",
        "2. Make lists for `numerical_features`, `ordinal_features`, and `nominal_features`.\n",
        "3. Preprocess the features. It is safer to make a copy of `X_train` using:\n",
        "   ```\n",
        "   X_train_copy = X_train.copy()\n",
        "   X_test_copy = X_test.copy()\n",
        "   ```\n",
        "   and preprocess `X_train_copy` instead.\n",
        "\n",
        "   **For nominal features, concat the one-hot encoded features using [`pd.concat(..., axis=1)`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) and drop the old nominal features from the dataframe.**\n",
        "4. Train Logistic Regression on the preprocessed `X_train_copy` and `y_train`.\n",
        "5. Evaluate the Model:\n",
        "    *   Make predictions on the preprocessed `X_test_copy`.\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n"
      ],
      "metadata": {
        "id": "CllpFvNBNYWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define feature groups (based on the UCI variable table)\n",
        "numerical_features = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n",
        "ordinal_features = ['education']\n",
        "nominal_features = ['job', 'marital', 'default', 'housing', 'loan',\n",
        "                    'contact', 'month', 'poutcome']\n",
        "\n",
        "# Make copies to keep original X_train, X_test safe\n",
        "X_train_copy = X_train.copy()\n",
        "X_test_copy = X_test.copy()\n",
        "\n",
        "# 2. Numerical: impute (in case of missing) + scale\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_train_num = num_imputer.fit_transform(X_train_copy[numerical_features])\n",
        "X_test_num = num_imputer.transform(X_test_copy[numerical_features])\n",
        "\n",
        "num_scaler = StandardScaler()\n",
        "X_train_num_scaled = num_scaler.fit_transform(X_train_num)\n",
        "X_test_num_scaled = num_scaler.transform(X_test_num)\n",
        "\n",
        "# 3. Ordinal: impute + ordinal encode (education)\n",
        "ord_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_train_ord_imp = ord_imputer.fit_transform(X_train_copy[ordinal_features])\n",
        "X_test_ord_imp = ord_imputer.transform(X_test_copy[ordinal_features])\n",
        "\n",
        "ordinal_encoder_full = OrdinalEncoder(categories=[education_categories])\n",
        "X_train_ord_enc = ordinal_encoder_full.fit_transform(X_train_ord_imp)\n",
        "X_test_ord_enc = ordinal_encoder_full.transform(X_test_ord_imp)\n",
        "\n",
        "# 4. Nominal: impute + one-hot encode\n",
        "nom_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_train_nom_imp = nom_imputer.fit_transform(X_train_copy[nominal_features])\n",
        "X_test_nom_imp = nom_imputer.transform(X_test_copy[nominal_features])\n",
        "\n",
        "onehot_encoder_full = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "X_train_nom_ohe = onehot_encoder_full.fit_transform(X_train_nom_imp)\n",
        "X_test_nom_ohe = onehot_encoder_full.transform(X_test_nom_imp)\n",
        "\n",
        "# 5. Concatenate all processed features\n",
        "X_train_processed = np.hstack([X_train_num_scaled, X_train_ord_enc, X_train_nom_ohe])\n",
        "X_test_processed = np.hstack([X_test_num_scaled, X_test_ord_enc, X_test_nom_ohe])\n",
        "\n",
        "print(\"X_train_processed shape:\", X_train_processed.shape)\n",
        "print(\"X_test_processed shape :\", X_test_processed.shape)\n",
        "\n",
        "# 6. Train Logistic Regression\n",
        "log_reg_bank = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "log_reg_bank.fit(X_train_processed, y_train)\n",
        "\n",
        "# 7. Evaluate on test set\n",
        "y_pred_bank = log_reg_bank.predict(X_test_processed)\n",
        "y_proba_bank = log_reg_bank.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bank))\n",
        "print(\"ROC-AUC :\", roc_auc_score(y_test, y_proba_bank))\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred_bank))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_bank))"
      ],
      "metadata": {
        "id": "vJBasyidiwqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Fashion-MNIST Dataset - Image Classification"
      ],
      "metadata": {
        "id": "m9qrm2DKRtgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fashion-MNIST Dataset\n",
        "\n",
        "The Fashion-MNIST dataset consists of 28x28 grayscale images of fashion items."
      ],
      "metadata": {
        "id": "kc8SZBvcS8_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(fm_X_train, fm_y_train), (fm_X_test, fm_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(f\"Fashion-MNIST Train data shape: {fm_X_train.shape}\")\n",
        "print(f\"Fashion-MNIST Train labels shape: {fm_y_train.shape}\")\n",
        "print(f\"Fashion-MNIST Test data shape: {fm_X_test.shape}\")\n",
        "print(f\"Fashion-MNIST Test labels shape: {fm_y_test.shape}\")"
      ],
      "metadata": {
        "id": "r0FQt8rlRgoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First image {fm_X_train[0]}\")\n",
        "print(f\"First label {fm_y_train[0]}\")"
      ],
      "metadata": {
        "id": "LXJ9EmcIVVrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Fashion-MNIST Images\n",
        "\n",
        "Let's see what these images look like."
      ],
      "metadata": {
        "id": "WwkQOE79TV70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist_class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "# Visualize the images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(fm_X_train[i], cmap='gray')\n",
        "    plt.xlabel(fashion_mnist_class_names[fm_y_train[i]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VI2ySCg2jGa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Preprocessing Images (Flatten and Scale)**\n",
        "\n",
        "Images are 2D arrays (matrices of pixels) and pixel values are integers from 0-255. For Logistic Regression, we need:\n",
        "*  **Flattening:** Convert each 28x28 image into a 1D array of 784 features.\n",
        "*  **Scaling:** Normalize pixel values from [0, 255] to [0, 1].\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.   **Flatten:** Use the `.reshape()` method (see [documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html)). For `fm_X_train_binary` (shape `(num_samples, 28, 28)`), you want to reshape it to `(num_samples, 28*28)`.\n",
        "2.  **Scale:** Divide the flattened pixel values by 255.0 to get values between 0 and 1.\n",
        "3.   **Train Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "    *   Fit the model on your *processed* `fm_X_train_scaled` and `fm_y_train`.\n",
        "4.   **Make Predictions:** Use `predict()` to make predictions on the *processed* `fm_X_test_scaled`.\n",
        "5.   **Print Classification Report:** Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n",
        "6.   **Visualize Misclassifications:**\n",
        "    *   Find the indices in `fm_X_test_binary` where your model made incorrect predictions (i.e., `fm_y_pred != fm_y_test`).\n",
        "    *   Select 5 of these misclassified images.\n",
        "    *   Plot these images (using `plt.imshow`). For each image, print its true label and its predicted label."
      ],
      "metadata": {
        "id": "cXvJB42xVrHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Flatten images: (n_samples, 28, 28) â†’ (n_samples, 784)\n",
        "n_train = fm_X_train.shape[0]\n",
        "n_test = fm_X_test.shape[0]\n",
        "\n",
        "fm_X_train_flat = fm_X_train.reshape(n_train, -1)\n",
        "fm_X_test_flat = fm_X_test.reshape(n_test, -1)\n",
        "\n",
        "# 2. Scale pixel values to [0, 1]\n",
        "fm_X_train_scaled = fm_X_train_flat.astype('float32') / 255.0\n",
        "fm_X_test_scaled = fm_X_test_flat.astype('float32') / 255.0\n",
        "\n",
        "print(\"fm_X_train_scaled shape:\", fm_X_train_scaled.shape)\n",
        "print(\"fm_X_test_scaled shape :\", fm_X_test_scaled.shape)\n",
        "\n",
        "# 3. Train Logistic Regression (multiclass)\n",
        "log_reg_fm = LogisticRegression(\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    multi_class='multinomial',\n",
        "    n_jobs=-1\n",
        ")\n",
        "log_reg_fm.fit(fm_X_train_scaled, fm_y_train)\n",
        "\n",
        "# 4. Predict on test set\n",
        "fm_y_pred = log_reg_fm.predict(fm_X_test_scaled)\n",
        "\n",
        "# 5. Evaluation\n",
        "print(\"Classification report (Fashion-MNIST):\")\n",
        "print(classification_report(fm_y_test, fm_y_pred, target_names=fashion_mnist_class_names))\n",
        "\n",
        "# 6. Visualize some misclassified images\n",
        "mis_idx = np.where(fm_y_pred != fm_y_test)[0]\n",
        "print(\"Number of misclassified examples:\", len(mis_idx))\n",
        "\n",
        "num_to_plot = 5\n",
        "plt.figure(figsize=(10, 2 * num_to_plot))\n",
        "for i in range(num_to_plot):\n",
        "    idx = mis_idx[i]\n",
        "    plt.subplot(num_to_plot, 2, 2 * i + 1)\n",
        "    plt.imshow(fm_X_test[idx], cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    true_label = fashion_mnist_class_names[fm_y_test[idx]]\n",
        "    pred_label = fashion_mnist_class_names[fm_y_pred[idx]]\n",
        "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "owI0bG_FjPBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: 20 Newsgroups Dataset - Text Classification"
      ],
      "metadata": {
        "id": "u7-q5FmnboVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load 20 Newsgroups Dataset\n",
        "\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics."
      ],
      "metadata": {
        "id": "6TDncAyyb6mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "news_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
        "\n",
        "X_train_news, y_train_news = news_train.data, news_train.target\n",
        "X_test_news, y_test_news = news_test.data, news_test.target\n",
        "\n",
        "print(f\"Number of training documents: {len(X_train_news)}\")\n",
        "print(f\"Number of test documents: {len(X_test_news)}\")\n",
        "print(f\"Categories: {news_train.target_names}\")"
      ],
      "metadata": {
        "id": "E91xZl5NbnpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Sample Document"
      ],
      "metadata": {
        "id": "tVCq09V2cfGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first document and its class\n",
        "print(\"First training document:\\n\")\n",
        "print(X_train_news[0])\n",
        "\n",
        "print(\"\\nClass index:\", y_train_news[0])\n",
        "print(\"Class name :\", news_train.target_names[y_train_news[0]])"
      ],
      "metadata": {
        "id": "Pi6z0BYdjhqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: Text Vectorization Demonstration with `TfidfVectorizer`"
      ],
      "metadata": {
        "id": "1vXXUdp7chsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{number of word }t\\text{ in } d}{\\text{number of words in } d} \\quad \\text{ and } \\quad\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{total number of documents}}{\\text{number of documents that contain word }t}\\right).\n",
        "$$"
      ],
      "metadata": {
        "id": "rwaq1igbi-Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the sample sentences\n",
        "sample_vec_output_sparse = vectorizer.fit_transform(sample_sentences)\n",
        "\n",
        "sample_vec_output_dense = sample_vec_output_sparse.toarray()\n",
        "\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(sample_vec_output_dense)"
      ],
      "metadata": {
        "id": "dvyNdIJ-jyDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Apply TF-IDF Vectorization to Full Dataset**\n",
        "\n",
        "Now, apply `TfidfVectorizer` to the actual training and testing datasets for the 20 Newsgroups classification task.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Initialize `TfidfVectorizer`:**\n",
        "    *   Initialize `TfidfVectorizer`. Use `stop_words='english'` to remove common words.\n",
        "2.  **Fit and Transform Training Data:**\n",
        "    *   Call `fit_transform()` on `X_train_news` to learn the vocabulary and transform the training text into TF-IDF features. Store the result in `X_train_vec`.\n",
        "3.  **Transform Test Data:**\n",
        "    *   Call `transform()` on `X_test_news` using the *already fitted* vectorizer. Store the result in `X_test_vec`. **Crucially, do not call `fit_transform()` on the test data!** This would cause data leakage.\n",
        "4.  **Initialize Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "5.  **Train the Model:**\n",
        "    *   Fit the model on your `X_train_vec` and `y_train_news`.\n",
        "6.  **Make Predictions:**\n",
        "    *   Make predictions using `predict()` on the `X_test_vec`.\n",
        "7.  **Evaluate the Model:**\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?"
      ],
      "metadata": {
        "id": "aCa_dcEDc-PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# 2. Fit and transform training data\n",
        "X_train_vec = tfidf_vectorizer.fit_transform(X_train_news)\n",
        "\n",
        "# 3. Transform test data (DO NOT fit again on test!)\n",
        "X_test_vec = tfidf_vectorizer.transform(X_test_news)\n",
        "\n",
        "print(\"X_train_vec shape:\", X_train_vec.shape)\n",
        "print(\"X_test_vec shape :\", X_test_vec.shape)\n",
        "\n",
        "# 4. Initialize Logistic Regression\n",
        "log_reg_news = LogisticRegression(solver='saga', max_iter=1000)\n",
        "\n",
        "# 5. Train the model\n",
        "log_reg_news.fit(X_train_vec, y_train_news)\n",
        "\n",
        "# 6. Make predictions on test data\n",
        "y_pred_news = log_reg_news.predict(X_test_vec)\n",
        "\n",
        "# 7. Evaluate the model\n",
        "print(\"Accuracy (20 Newsgroups):\", accuracy_score(y_test_news, y_pred_news))\n",
        "print(\"\\nClassification report (20 Newsgroups):\")\n",
        "print(classification_report(y_test_news, y_pred_news, target_names=news_train.target_names))"
      ],
      "metadata": {
        "id": "nK3Tl9fLkRuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}